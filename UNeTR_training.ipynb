{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import struct\n",
    "import json\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "\n",
    "from networks import RegModel,ShufflePermutation\n",
    "\n",
    "from utils import *\n",
    "\n",
    "if is_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"10\"\n",
    "\n",
    "tasks=[str(x) for x in sys.argv[1:]]\n",
    "\n",
    "print('Training Following Tasks',tasks, '#', len(tasks))\n",
    "\n",
    "results={}\n",
    "\n",
    "for task in tasks:\n",
    "    results[task]={}\n",
    "skip_stage = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_seg(task_name, split ='train'):\n",
    "    if task_name == 'AbdomenCTCT':\n",
    "        with open('data_compressed/AbdomenCTCT/AbdomenCTCT_dataset.json') as f:\n",
    "            dataset=json.load(f)\n",
    "\n",
    "        image_shape=dataset['tensorImageShape']['0']\n",
    "        num_labels=len(dataset['labels']['0'])\n",
    "        H,W,D=image_shape\n",
    "\n",
    "        if split == 'train':\n",
    "            mode = 'Tr'\n",
    "            cases = [str(x).zfill(4) for x in sorted(list(range(31)[2::3])+list(range(31)[3::3]))]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(f'data_compressed/AbdomenCTCT/labelsTr/AbdomenCTCT_{case}_0000.nii.gz').get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "        \n",
    "        elif split == 'val':\n",
    "            mode = 'Tr'\n",
    "            cases = [str(x).zfill(4) for x in list(range(31)[1::3])]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,1,H,W,D).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = torch.from_numpy(nib.load(f'data_compressed/AbdomenCTCT/labelsTr/AbdomenCTCT_{case}_0000.nii.gz').get_fdata()).long().unsqueeze(0)\n",
    "\n",
    "        elif split == 'val_pred':\n",
    "            mode = 'Tr'\n",
    "            cases = [str(x).zfill(4) for x in list(range(31)[1::3])]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(f'data_compressed/AbdomenCTCT/predictedlabelsTr/AbdomenCTCT_{case}_0000.nii.gz').get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "    elif task_name == 'AMOS':\n",
    "        with open('data_compressed/AMOS/AMOS_dataset.json') as f:\n",
    "            dataset=json.load(f)\n",
    "\n",
    "        image_shape=dataset['tensorImageShape']['0']\n",
    "        num_labels=len(dataset['labels']['0'])\n",
    "        H,W,D=image_shape\n",
    "        lst = ['0507', '0508', '0510', '0514', '0517', '0518', '0522', '0530', '0532', '0538', '0540', '0541', '0551', '0555', '0557', '0571', '0578', '0580', '0582', '0584', '0585', '0586', '0587', '0588', '0589', '0590', '0592', '0594', '0595', '05self.base_size*2', '0597', '0599']\n",
    "        if split == 'train':\n",
    "            mode = 'Tr'\n",
    "            cases = lst [:-6]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(f'data_compressed/AMOS/labelsTr/AMOS_{case}_0000.nii.gz').get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "        \n",
    "        elif split == 'val':\n",
    "            mode = 'Tr'\n",
    "            cases = lst [-6:]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,1,H,W,D).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = torch.from_numpy(nib.load(f'data_compressed/AMOS/labelsTr/AMOS_{case}_0000.nii.gz').get_fdata()).long().unsqueeze(0)\n",
    "\n",
    "        elif split == 'val_pred':\n",
    "            mode = 'Tr'\n",
    "            cases = lst [-6:]\n",
    "            num_train=len(cases)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            mappingtensor = torch.LongTensor([ 0, 12, 11,  8,  0,  0,  0, 13,  5,  4,  9,  3,  2,  6, 10,  1,  7])\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(mappingtensor[torch.from_numpy(nib.load(f'data_compressed/AMOS_pred/predictedlabelsTr/AMOS_{case}_0000.nii.gz').get_fdata()).long()].unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "    \n",
    "    elif task_name == 'TS_Skeletal':\n",
    "\n",
    "        list_data=sorted(glob.glob(f'data_compressed/TS_Skeletal/labels/*nii.gz'))\n",
    "        if split == 'train':\n",
    "            cases = list_data[:27]\n",
    "            num_train,num_labels,H,W,D = (len(cases),29,256,160,256)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(case).get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "        elif split == 'val':\n",
    "            cases = list_data[27:]\n",
    "            num_train,num_labels,H,W,D = (len(cases),29,256,160,256)\n",
    "            labels = torch.zeros(num_train,1,H,W,D).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = torch.from_numpy(nib.load(case).get_fdata()).long().unsqueeze(0)\n",
    "\n",
    "        elif split == 'val_pred':\n",
    "            cases = list_data[27:]\n",
    "            num_train,num_labels,H,W,D = (len(cases),29,256,160,256)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(case).get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "    elif task_name == 'SilverCorpus':\n",
    "        mappingtensor =torch.LongTensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4,\n",
    "        4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "        mapping_dict={\n",
    "        \"inferior_vena_cava\" : 2,\n",
    "        \"aorta\" : 1,\n",
    "        \"pulmonary_artery\" : 3,\n",
    "        \"heart_myocardium\": 4,\n",
    "        \"heart_atrium_left\": 4,\n",
    "        \"heart_ventricle_left\": 4,\n",
    "        \"heart_atrium_right\": 4,\n",
    "        \"heart_ventricle_right\":4,\n",
    "        \"lung_upper_lobe_left\":5,\n",
    "        \"lung_lower_lobe_left\":6,\n",
    "        \"lung_upper_lobe_right\":7,\n",
    "        \"lung_middle_lobe_right\":8,\n",
    "        \"lung_lower_lobe_right\":9\n",
    "        }\n",
    "\n",
    "        if split == 'train':\n",
    "            cases = [13, 24, 26, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 55, 57, 58, 59, 60, 62, 64, 65, 69, 70, 71, 72, 75, 76, 77, 78, 84, 91, 93, 98, 141]\n",
    "            num_train,num_labels,H,W,D = (len(cases),mappingtensor.max().item()+1,256,192,288)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                case_ = f'data_compressed/SilverCorpus/silver{str(case).zfill(3)}.nii.gz'\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(mappingtensor[torch.from_numpy(nib.load(case_).get_fdata()).long().unsqueeze(0)],num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "        if split == 'val':\n",
    "            cases = [12, 36, 49, 68, 97]\n",
    "            num_train,num_labels,H,W,D = (len(cases),mappingtensor.max().item()+1,256,192,288)\n",
    "            labels = torch.zeros(num_train,1,H,W,D).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                case_ = f'data_compressed/SilverCorpus/silver{str(case).zfill(3)}.nii.gz'\n",
    "                labels[i] = mappingtensor[torch.from_numpy(nib.load(case_).get_fdata()).long().unsqueeze(0)]\n",
    "            \n",
    "        if split == 'val_pred':\n",
    "            cases = [12, 36, 49, 68, 97]\n",
    "            num_train,num_labels,H,W,D = (len(cases),mappingtensor.max().item()+1,256,192,288)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                case_ = f'data_compressed/SilverCorpus/silver{str(case).zfill(3)}.nii.gz'\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(mappingtensor[torch.from_numpy(nib.load(case_).get_fdata()).long().unsqueeze(0)],num_labels).permute(0,4,1,2,3),2)\n",
    "\n",
    "\n",
    "    print('loaded', task_name, split)   \n",
    "    return labels, (num_train,num_labels,H,W,D)\n",
    "\n",
    "def get_val_pairs(B):\n",
    "    ii_all = torch.empty(0,2).long()\n",
    "    for i in range(B):\n",
    "        for j in range(B):\n",
    "            if(i<j):\n",
    "                ii_all = torch.cat((ii_all,torch.tensor([i,j]).long().view(1,2)),0)\n",
    "    return ii_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamReg(mind_fix,mind_mov,dense_flow):\n",
    "\n",
    "    \n",
    "    if(dense_flow.shape[-1]==3):\n",
    "        dense_flow = dense_flow.permute(0,4,1,2,3)\n",
    "    \n",
    "    H,W,D = dense_flow[0,0].shape\n",
    "    \n",
    "    disp_hr = dense_flow.cuda().flip(1)*torch.tensor([H-1,W-1,D-1]).cuda().view(1,3,1,1,1)/2\n",
    "    with torch.enable_grad(): \n",
    "        grid_sp = 2\n",
    "\n",
    "       \n",
    "        disp_lr = F.interpolate(disp_hr,size=(H//grid_sp,W//grid_sp,D//grid_sp),mode='trilinear',align_corners=False)\n",
    "        net = nn.Sequential(nn.Conv3d(3,1,(H//grid_sp,W//grid_sp,D//grid_sp),bias=False))\n",
    "        net[0].weight.data[:] = disp_lr.float().cpu().data/grid_sp\n",
    "        net.cuda()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=1)\n",
    "        grid0 = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,H//grid_sp,W//grid_sp,D//grid_sp),align_corners=False)\n",
    "        #run Adam optimisation with diffusion regularisation and B-spline smoothing\n",
    "        lambda_weight = .65\n",
    "        for iter in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            disp_sample = F.avg_pool3d(F.avg_pool3d(F.avg_pool3d(net[0].weight,3,stride=1,padding=1),3,stride=1,padding=1),\\\n",
    "                                       3,stride=1,padding=1).permute(0,2,3,4,1)\n",
    "            reg_loss = lambda_weight*((disp_sample[0,:,1:,:]-disp_sample[0,:,:-1,:])**2).mean()+\\\n",
    "            lambda_weight*((disp_sample[0,1:,:,:]-disp_sample[0,:-1,:,:])**2).mean()+\\\n",
    "            lambda_weight*((disp_sample[0,:,:,1:]-disp_sample[0,:,:,:-1])**2).mean()\n",
    "            scale = torch.tensor([(H//grid_sp-1)/2,(W//grid_sp-1)/2,(D//grid_sp-1)/2]).cuda().unsqueeze(0)\n",
    "            grid_disp = grid0.view(-1,3).cuda().float()+((disp_sample.view(-1,3))/scale).flip(1).float()\n",
    "            patch_mov_sampled = F.grid_sample(mind_mov.cuda().float(),grid_disp.view(1,H//grid_sp,W//grid_sp,D//grid_sp,3).cuda()\\\n",
    "                                              ,align_corners=False,mode='bilinear')\n",
    "            sampled_cost = (patch_mov_sampled-mind_fix.cuda()).pow(2).mean(1)*12\n",
    "            loss = sampled_cost.mean()\n",
    "            (loss+reg_loss).backward()\n",
    "            optimizer.step()\n",
    "        fitted_grid = disp_sample.permute(0,4,1,2,3).detach()\n",
    "        disp_hr = F.interpolate(fitted_grid*grid_sp,size=(H,W,D),mode='trilinear',align_corners=False)\n",
    "        \n",
    "    disp_smooth = F.avg_pool3d(F.avg_pool3d(F.avg_pool3d(disp_hr,3,padding=1,stride=1),3,padding=1,stride=1),3,padding=1,stride=1)\n",
    "\n",
    "\n",
    "    disp_hr = torch.flip(disp_smooth/torch.tensor([H-1,W-1,D-1]).view(1,3,1,1,1).cuda()*2,[1])\n",
    "    return disp_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=['SilverCorpus','TS_Skeletal']\n",
    "if False:\n",
    "    results={}\n",
    "    for task in tasks:\n",
    "        results[task]={}#\n",
    "\n",
    "    train_label = []; train_shapes = []\n",
    "    iterate_list = [train_label, train_shapes]\n",
    "\n",
    "    for t in tasks:\n",
    "        data=load_dataset_seg(t,split='train')\n",
    "        for x, lst in zip(data, iterate_list):\n",
    "            lst.append(x)\n",
    "\n",
    "\n",
    "    val_label = []; val_shapes = []\n",
    "    iterate_list = [val_label, val_shapes]\n",
    "\n",
    "    for t in tasks:\n",
    "        data=load_dataset_seg(t,split='val')\n",
    "        for x, lst in zip(data, iterate_list):\n",
    "            lst.append(x)\n",
    "\n",
    "\n",
    "    pred_label = []; pred_shapes = []\n",
    "    iterate_list = [pred_label, pred_shapes]\n",
    "    for t in tasks:\n",
    "        data=load_dataset_seg(t,split='val_pred')\n",
    "        for x, lst in zip(data, iterate_list):\n",
    "            lst.append(x)\n",
    "\n",
    "\n",
    "\n",
    "##testing\n",
    "else:\n",
    "    train_label = []\n",
    "    train_shapes = []\n",
    "    sc_mappingtensor =torch.LongTensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4,\n",
    "            4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "            0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "    for t in tasks:\n",
    "        if t == 'SilverCorpus':\n",
    "            cases = [13, 24]#, 26, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 53, 54, 55, 57, 58, 59, 60, 62, 64, 65, 69, 70, 71, 72, 75, 76, 77, 78, 84, 91, 93, 98, 141]\n",
    "            num_train,num_labels,H,W,D = (len(cases),sc_mappingtensor.max().item()+1,256,192,288)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                case_ = f'data_compressed/SilverCorpus/silver{str(case).zfill(3)}.nii.gz'\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(sc_mappingtensor[torch.from_numpy(nib.load(case_).get_fdata()).long().unsqueeze(0)],num_labels).permute(0,4,1,2,3),2)\n",
    "        elif t == 'TS_Skeletal':\n",
    "            #list_data=sorted(glob.glob(f'data_compressed/TS_Skeletal/labels/*nii.gz'))\n",
    "            cases = [4,224]\n",
    "            num_train,num_labels,H,W,D = (len(cases),29,256,160,256)\n",
    "            labels = torch.zeros(num_train,num_labels,H//2,W//2,D//2).pin_memory()\n",
    "            for i,case in tqdm(enumerate(cases),total=num_train):\n",
    "                case_ = f'data_compressed/TS_Skeletal/labels/TS_Sk_{str(case).zfill(4)}_0000.nii.gz'\n",
    "                labels[i] = F.avg_pool3d(F.one_hot(torch.from_numpy(nib.load(case_).get_fdata()).long().unsqueeze(0),num_labels).permute(0,4,1,2,3),2)\n",
    "        train_label.append(labels)\n",
    "        train_shapes.append((num_train,num_labels,H,W,D))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edt_train_label=[]\n",
    "\n",
    "for dataset,task in enumerate(tasks):\n",
    "    B,num_classes,H,W,D = train_label[dataset].shape\n",
    "\n",
    "    tmp=torch.zeros(B,1,H,W,D)\n",
    "    for i in tqdm(range(B)):\n",
    "        for ii in range(num_classes):\n",
    "            edt = torch.from_numpy(distance_transform_edt((train_label[dataset][i,ii]).cpu().squeeze())).float()\n",
    "            tmp[i,0]+=(7-nn.ELU()(7-edt))/7\n",
    "    edt_train_label.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "\n",
    "class RegModel(nn.Module):\n",
    "    def __init__(self,in_ch=14,base_size=64):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "\n",
    "        self.base_size = base_size\n",
    "        self.feat1 = nn.Sequential(nn.Conv3d(self.in_ch,16,5,padding=2,stride=2),nn.Upsample(size=[self.base_size*2,self.base_size*2,self.base_size*2],mode='trilinear'),nn.InstanceNorm3d(16),nn.ReLU(),\\\n",
    "                                    nn.Conv3d(16,32,3,padding=1),nn.InstanceNorm3d(32),nn.ReLU(),nn.Conv3d(32,32,3,padding=1))\n",
    "        self.feat2 = nn.Sequential(nn.Conv3d(32,64,3,padding=1,stride=2),nn.InstanceNorm3d(64),nn.ReLU(),\\\n",
    "                                    nn.Conv3d(64,64,3,padding=1),nn.InstanceNorm3d(64),nn.ReLU(),nn.Conv3d(64,64,3,padding=1))\n",
    "\n",
    "        self.reg1 = monai.networks.nets.swin_unetr.SwinUNETR(img_size=(self.base_size*2,self.base_size*2,self.base_size*2),in_channels=64,out_channels=3,spatial_dims=3)\n",
    "        self.reg2 = monai.networks.nets.swin_unetr.SwinUNETR(img_size=(self.base_size,self.base_size,self.base_size),in_channels=128,out_channels=3,spatial_dims=3)\n",
    " \n",
    "    def forward(self, x,y,level=1):\n",
    "        H,W,D = x.shape[-3:]\n",
    "        grid = F.affine_grid(torch.eye(3,4).unsqueeze(0),(1,1,self.base_size*3,self.base_size*3,self.base_size*3))\n",
    "\n",
    "        x2 = self.feat2(self.feat1(x))\n",
    "        y2 = self.feat2(self.feat1(y))\n",
    "        #reg+interpolate+smooth\n",
    "        disp2 = F.interpolate(self.reg2(F.interpolate(torch.cat((x2,y2),1),size=(self.base_size,self.base_size,self.base_size),mode='trilinear')),size=(self.base_size*3,self.base_size*3,self.base_size*3),mode='trilinear')\n",
    "        disp2 = F.avg_pool3d(F.avg_pool3d(disp2,5,stride=1,padding=2),5,stride=1,padding=2)\n",
    "            \n",
    "        if(level==2):\n",
    "            y_ = F.grid_sample(y,grid.to(disp2.device)+disp2.permute(0,2,3,4,1))\n",
    "            x1 = self.feat1(x)\n",
    "            y1 = self.feat1(y_)\n",
    "            #smooth\n",
    "            disp1 = F.interpolate(self.reg1(F.interpolate(torch.cat((x1,y1),1),size=(self.base_size*2,self.base_size*2,self.base_size*2),mode='trilinear')),size=(self.base_size*3,self.base_size*3,self.base_size*3),mode='trilinear')\n",
    "            disp1 = F.avg_pool3d(F.avg_pool3d(disp1,5,stride=1,padding=2),5,stride=1,padding=2)\n",
    "            disp2 += disp1\n",
    "\n",
    "    \n",
    "        return disp2\n",
    "    \n",
    "trans_size = 64\n",
    "model=RegModel(1,base_size=64).cuda()\n",
    "import torchinfo\n",
    "print(torchinfo.summary(model,input_size=((1,1,trans_size*3,trans_size*3,trans_size*3),(1,1,trans_size*3,trans_size*3,trans_size*3)),depth=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 not in skip_stage:\n",
    "    model = RegModel(in_ch = 1, base_size= 64)\n",
    "    model.cuda()\n",
    "    repeats = 3\n",
    "    iterations =2000\n",
    "    run_dataset=torch.randint(0,len(tasks),[repeats,iterations])\n",
    "\n",
    "    trans_size =64\n",
    "\n",
    "    for repeat in range(repeats):\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        ramp_up = torch.sigmoid(torch.linspace(-5,25,iterations))\n",
    "\n",
    "        run_loss = torch.zeros(iterations,2)\n",
    "        run_val = torch.zeros(iterations//10)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        with trange(iterations) as pbar:\n",
    "            for i in pbar:     \n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    dataset=run_dataset[repeat,i]\n",
    "                    B,C,H,W,D = train_shapes[dataset]\n",
    "                    ii = torch.randperm(B)[:2]\n",
    "\n",
    "                    grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3)).cuda()\n",
    "                    \n",
    "                    affine = F.affine_grid((.07*ramp_up[i]*torch.randn(1,3,4)+torch.eye(3,4).unsqueeze(0)).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3),align_corners=False)\n",
    "                    affine0 = F.affine_grid((.07*ramp_up[i]*torch.randn(1,3,4)+torch.eye(3,4).unsqueeze(0)).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3),align_corners=False)\n",
    "                    fix_aff = F.grid_sample(edt_train_label[dataset][ii[:1]].cuda(),affine,align_corners=False)\n",
    "                    fix_aff_img =  F.grid_sample(train_label[dataset][ii[:1]].cuda(),affine,align_corners=False)\n",
    "                    mov_aff = F.grid_sample(edt_train_label[dataset][ii[1:2]].cuda(),affine0,align_corners=False)\n",
    "                    mov_aff_img = F.grid_sample(train_label[dataset][ii[1:2]].cuda(),affine0,align_corners=False)\n",
    "\n",
    "                    disp = model(fix_aff,mov_aff,level=int(i>iterations//2)+1)\n",
    "                    warped_img = F.grid_sample(mov_aff_img,grid+disp.permute(0,2,3,4,1),padding_mode='border',align_corners=False)\n",
    "                    loss = (1-soft_dice(fix_aff_img,warped_img)).mean()\n",
    "                scaler.scale(loss).backward()\n",
    "                #scaler.unscale_(optimizer)\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                run_loss[i,0] = (1-soft_dice(fix_aff_img,mov_aff_img)).mean()\n",
    "                run_loss[i,1] = loss.item()\n",
    "\n",
    "                str1 = f\"d: {dataset.item()}, iter: {i}, loss: {'%0.3f'%run_loss[i-25:i-1,1].mean()} | {'%0.3f'%run_loss[i-25:i-1,0].mean()}, runtime: {'%0.1f'%(time.time()-t0)} sec, gpumem/max: {'%0.2f'%(torch.cuda.max_memory_allocated()*1e-9)} GB\"\n",
    "                pbar.set_description(str1)\n",
    "                        #print('dice',dice_val)\n",
    "\n",
    "        print(f\"Repeat {repeat}, Last 100 Losses {'%0.3f'%run_loss[i-101:i-1,1].mean()}\")\n",
    "        plt.plot(F.avg_pool1d(F.avg_pool1d(run_loss[:,1].view(1,1,-1),15,stride=3),15,stride=1).squeeze())\n",
    "\n",
    "torch.save(model,f'transformer_models/{tasks[0]}_{tasks[1]}_edt_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    edt_pred_label=[]\n",
    "    for dataset,task in enumerate(tasks):\n",
    "        B,num_classes,H,W,D = pred_label[dataset].shape\n",
    "        tmp=torch.zeros(B,1,H,W,D)\n",
    "        for i in tqdm(range(B)):\n",
    "            for ii in range(num_classes):\n",
    "                edt = torch.from_numpy(distance_transform_edt((pred_label[dataset][i,ii]).cpu().squeeze())).float()\n",
    "                tmp[i,0]+=(7-nn.ELU()(7-edt))/7\n",
    "        edt_pred_label.append(tmp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.eval()\n",
    "    val_tres=[]\n",
    "\n",
    "    for dataset,task in enumerate(tasks):\n",
    "        B,num_classes,H,W,D = val_label[dataset].shape\n",
    "        num_classes = val_label[dataset].int().max().item()\n",
    "\n",
    "        ii_all = get_val_pairs(B)\n",
    "        val_dice=torch.zeros(ii_all.shape[0],5)\n",
    "        with tqdm(total=ii_all.shape[0], file=sys.stdout) as pbar:\n",
    "            for ii in range(ii_all.shape[0]):\n",
    "                grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,64*3,64*3,64*3)).cuda()\n",
    "\n",
    "                FIX = F.grid_sample(edt_pred_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda(),grid,align_corners=False)\n",
    "                MOV = F.grid_sample(edt_pred_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda(),grid,align_corners=False)\n",
    "                \n",
    "                GT_FIX = val_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda()\n",
    "                GT_MOV = val_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda()\n",
    "\n",
    "                grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,H,W,D)).cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    disp = model(FIX,MOV,level=2)\n",
    "                    SEG_WARP= F.grid_sample(GT_MOV,grid+F.grid_sample(disp,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                    val_dice[ii,0]=dice_coeff(GT_FIX,GT_MOV,num_classes).mean()\n",
    "                    val_dice[ii,1]=dice_coeff(GT_FIX,SEG_WARP,num_classes).mean()\n",
    "                \n",
    "                disp2=AdamReg(FIX,MOV,F.interpolate(disp,scale_factor=2,mode='trilinear').permute(0,2,3,4,1))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+F.grid_sample(disp2,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,2]=dice_coeff(GT_FIX,SEG_WARP2,num_classes).mean()\n",
    "    #\n",
    "                disp2=AdamReg(FIX,MOV,torch.zeros([1,3,192*2,192*2,192*2]))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+F.grid_sample(disp2,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,3]=dice_coeff(GT_FIX,SEG_WARP2,num_classes).mean()\n",
    "\n",
    "                FIX = pred_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda()\n",
    "                MOV = pred_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda()\n",
    "                disp2=AdamReg(FIX,MOV,F.grid_sample(disp,grid).permute(0,2,3,4,1))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+disp2.permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,4]=dice_coeff(GT_FIX,SEG_WARP2,14).mean()\n",
    "\n",
    "\n",
    "                str1 = f\"case: {ii}, before: {'%0.3f'%val_dice[ii,0].item()} | after: {'%0.3f'%val_dice[ii,1].item()}| adam: {'%0.3f'%val_dice[ii,2].item()}\"\n",
    "                pbar.set_description(str1); pbar.update(1)\n",
    "        results[task]['edt']=val_dice.mean(0).tolist()\n",
    "        \n",
    "        print(task,val_dice.mean(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2 not in skip_stage:\n",
    "    #model=torch.load(f'transformer_models/{tasks[0]}_{tasks[1]}_edt_model.pth').cuda()\n",
    "\n",
    "    adapt=ShufflePermutation().cuda()\n",
    "    model.train(); adapt.train()\n",
    "\n",
    "\n",
    "    repeats = 3\n",
    "    iterations = 2000\n",
    "    run_dataset=torch.randint(0,len(tasks),[repeats,iterations])\n",
    "    trans_size = 64\n",
    "\n",
    "    for repeat in range(repeats):\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(model.parameters())+list(adapt.parameters()),lr=0.001)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        ramp_up = torch.sigmoid(torch.linspace(-5,25,iterations))\n",
    "\n",
    "        run_loss = torch.zeros(iterations,2)\n",
    "        run_val = torch.zeros(iterations//10)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        with trange(iterations) as pbar:\n",
    "            for i in pbar:     \n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    dataset=run_dataset[repeat,i]\n",
    "\n",
    "                    B,C,H,W,D = train_shapes[dataset]\n",
    "                    ii = torch.randperm(B)[:2]\n",
    "\n",
    "\n",
    "                    grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3)).cuda()\n",
    "                    \n",
    "                    affine = F.affine_grid((.07*ramp_up[i]*torch.randn(1,3,4)+torch.eye(3,4).unsqueeze(0)).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3),align_corners=False)\n",
    "                    affine0 = F.affine_grid((.07*ramp_up[i]*torch.ones(1,3,4)+torch.eye(3,4).unsqueeze(0)).cuda(),(1,1,trans_size*3,trans_size*3,trans_size*3),align_corners=False)\n",
    "                    fix_aff = F.grid_sample(train_label[dataset][ii[:1]].cuda(),affine,align_corners=False)\n",
    "                    fix_aff_img =  F.grid_sample(train_label[dataset][ii[:1]].cuda(),affine,align_corners=False)\n",
    "                    mov_aff = F.grid_sample(train_label[dataset][ii[1:2]].cuda(),affine0,align_corners=False)\n",
    "                    mov_aff_img = F.grid_sample(train_label[dataset][ii[1:2]].cuda(),affine0,align_corners=False)\n",
    "\n",
    "                    disp = model(adapt(fix_aff),adapt(mov_aff),level=2)#int(i>iterations//2)+1)\n",
    "                    warped_img = F.grid_sample(mov_aff_img,grid+disp.permute(0,2,3,4,1),padding_mode='border',align_corners=False)\n",
    "                    loss = (1-soft_dice(fix_aff_img,warped_img)).mean()\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                #scaler.unscale_(optimizer)\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                run_loss[i,0] = (1-soft_dice(fix_aff_img,mov_aff_img)).mean()\n",
    "                run_loss[i,1] = loss.item()\n",
    "\n",
    "                str1 = f\"d: {dataset.item()}, iter: {i}, loss: {'%0.3f'%run_loss[i-25:i-1,1].mean()} | {'%0.3f'%run_loss[i-25:i-1,0].mean()}, runtime: {'%0.1f'%(time.time()-t0)} sec, gpumem/max: {'%0.2f'%(torch.cuda.max_memory_allocated()*1e-9)} GB\"\n",
    "                pbar.set_description(str1)\n",
    "\n",
    "        print(f\"Repeat {repeat}, Last 100 Losses {'%0.3f'%run_loss[i-101:i-1,1].mean()}\")\n",
    "        plt.plot(F.avg_pool1d(F.avg_pool1d(run_loss.view(1,1,-1),15,stride=3),15,stride=1).squeeze())\n",
    "\n",
    "    torch.save([model,adapt],f'transformer_models/{tasks[0]}_{tasks[1]}_adapt_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.eval()\n",
    "    adapt.eval()\n",
    "    val_tres=[]\n",
    "\n",
    "    for dataset,task in enumerate(tasks):\n",
    "        B,num_classes,H,W,D = val_label[dataset].shape\n",
    "        num_classes = val_label[dataset].int().max().item()\n",
    "\n",
    "        ii_all = get_val_pairs(B)\n",
    "        val_dice=torch.zeros(ii_all.shape[0],5)\n",
    "        with tqdm(total=ii_all.shape[0], file=sys.stdout) as pbar:\n",
    "            for ii in range(ii_all.shape[0]):\n",
    "                grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,64*3,64*3,64*3)).cuda()\n",
    "\n",
    "                FIX = adapt(F.grid_sample(pred_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda(),grid,align_corners=False))\n",
    "                MOV = adapt(F.grid_sample(pred_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda(),grid,align_corners=False))\n",
    "                \n",
    "                GT_FIX = val_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda()\n",
    "                GT_MOV = val_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda()\n",
    "\n",
    "                grid = F.affine_grid(torch.eye(3,4).unsqueeze(0).cuda(),(1,1,H,W,D)).cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    disp = model(FIX,MOV,level=2)\n",
    "                    SEG_WARP= F.grid_sample(GT_MOV,grid+F.grid_sample(disp,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                    val_dice[ii,0]=dice_coeff(GT_FIX,GT_MOV,num_classes).mean()\n",
    "                    val_dice[ii,1]=dice_coeff(GT_FIX,SEG_WARP,num_classes).mean()\n",
    "                \n",
    "                disp2=AdamReg(FIX,MOV,F.interpolate(disp,scale_factor=2,mode='trilinear').permute(0,2,3,4,1))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+F.grid_sample(disp2,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,2]=dice_coeff(GT_FIX,SEG_WARP2,num_classes).mean()\n",
    "    #\n",
    "                disp2=AdamReg(FIX,MOV,torch.zeros([1,3,192*2,192*2,192*2]))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+F.grid_sample(disp2,grid).permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,3]=dice_coeff(GT_FIX,SEG_WARP2,num_classes).mean()\n",
    "\n",
    "                FIX = pred_label[dataset][ii_all[ii,0]:ii_all[ii,0]+1].cuda()\n",
    "                MOV = pred_label[dataset][ii_all[ii,1]:ii_all[ii,1]+1].cuda()\n",
    "                disp2=AdamReg(FIX,MOV,F.grid_sample(disp,grid).permute(0,2,3,4,1))\n",
    "                SEG_WARP2= F.grid_sample(GT_MOV,grid+disp2.permute(0,2,3,4,1),padding_mode='border',mode='nearest',align_corners=False)\n",
    "                val_dice[ii,4]=dice_coeff(GT_FIX,SEG_WARP2,14).mean()\n",
    "\n",
    "\n",
    "                str1 = f\"case: {ii}, before: {'%0.3f'%val_dice[ii,0].item()} | after: {'%0.3f'%val_dice[ii,1].item()}| adam: {'%0.3f'%val_dice[ii,2].item()}\"\n",
    "                pbar.set_description(str1); pbar.update(1)\n",
    "        results[task]['edt']=val_dice.mean(0).tolist()\n",
    "        \n",
    "        print(task,val_dice.mean(0))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ea7b25c166a17916b076a2f2c2a4306ef4fff6630470fa2690364fd40048f96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
